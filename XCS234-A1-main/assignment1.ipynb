{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.array(\n",
    "    [\n",
    "        [\"terminal\",\"terminal\",\"terminal\",\"terminal\",\"terminal\"],\n",
    "        [\"unshaded\",\"unshaded\",\"terminal\",\"unshaded\",\"unshaded\"],\n",
    "        [\"unshaded\",\"unshaded\",\"terminal\",\"unshaded\",\"unshaded\"],\n",
    "        [\"unshaded\",\"unshaded\",\"unshaded\",\"unshaded\",\"goal\"],\n",
    "        [\"unshaded\",\"terminal\",\"unshaded\",\"unshaded\",\"unshaded\"],\n",
    "        [\"unshaded\",\"terminal\",\"unshaded\",\"unshaded\",\"unshaded\"],\n",
    "        [\"terminal\",\"terminal\",\"terminal\",\"terminal\",\"terminal\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROW, N_COL = label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "r_g = 5\n",
    "r_r = -5\n",
    "\n",
    "s0 = 2\n",
    "actions = { \"rightup\" : np.array([-1,1]), \"rightdown\" : np.array([1,1]) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_coordinates( state ):\n",
    "    state -= 1\n",
    "    column = int( state / N_ROW)\n",
    "    row = state - column * N_ROW\n",
    "    return row, column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinates_to_state ( row, column ):\n",
    "    return (column * N_ROW + row) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamics ( state_1, action_1 ):\n",
    "    row_1, col_1 = state_to_coordinates( state_1 )\n",
    "    label = label[row_1][col_1]\n",
    "\n",
    "    if label in [ \"terminal\", \"goal\" ]:\n",
    "        return state_1\n",
    "    \n",
    "    row_2 = row_1 + actions[action_1][0]\n",
    "    col_2 = col_1 + actions[action_1][1]\n",
    "\n",
    "    if ( row_2 > N_ROW-1 ) or ( row_2 < 0 ) or ( col_2 > N_COL-1 ) or ( col_2 < 0 ):\n",
    "        return coordinates_to_state( row_1+1, col_1 )\n",
    "    else:\n",
    "        return coordinates_to_state( row_2, col_2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(root, arr, ans):\n",
    "    arr.append(root)\n",
    "    left =  dynamics ( root, \"rightup\" )\n",
    "    right =  dynamics ( root, \"rightdown\" )\n",
    " \n",
    "    if root == left or root == right:\n",
    "        # This will be only true when the node is leaf node and hence we will update our ans array by inserting array arr which have one unique path from root to leaf\n",
    "        ans.append(arr.copy())\n",
    "        del arr[-1]\n",
    "        # after that we will return since we don't want to check after leaf node\n",
    "        return\n",
    " \n",
    "    # recursively going left and right until we find the leaf and updating the arr and ans array simultaneously\n",
    "    if left != right:\n",
    "        helper(left, arr, ans)\n",
    "        helper(right, arr, ans)\n",
    "    else:\n",
    "        helper(left, arr, ans)\n",
    "    del arr[-1]\n",
    " \n",
    " \n",
    "def Paths(root):\n",
    "    ans = [] # creating answer in which each element is a array having one unique path from root to leaf\n",
    "    arr = [] # arr is a array which will have one unique path from root to leaf at a time.arr will be updated recursively\n",
    "    helper(root, arr, ans) # after helper function call our ans array updated with paths so we will return ans array\n",
    "    return ans\n",
    " \n",
    "def printArray(paths, reward):\n",
    "    optimal_reward = 35*-5\n",
    "    len_optimal = 35\n",
    "    visited_state = set()\n",
    "    goal_optimal_reward = 35*-5\n",
    "    len_goal_optimal = 35\n",
    "    for path in paths:\n",
    "        state_list = []\n",
    "        label_list = []\n",
    "        reward_list = []\n",
    "\n",
    "        for state in path:\n",
    "            visited_state.add(state)\n",
    "            state_list.append(str(state))\n",
    "            row,col = state_to_coordinates( state )\n",
    "            label = flappyworld1[row][col]\n",
    "            label_list.append(label)\n",
    "            reward_list.append(str(reward[label]))\n",
    "        \n",
    "        print(\"path : \", end=\"\")\n",
    "        print(\" ---> \".join(state_list), end=\"\")\n",
    "        print(\" : length of shortest path : \" + str(len(state_list)))\n",
    "        print(\"label : \", end=\"\")\n",
    "        print(\" ---> \".join(label_list))\n",
    "        print(\"reward : \", end=\"\")\n",
    "        print(\" + \".join(reward_list), end=\"\")\n",
    "        total_reward = sum( list(map(int, reward_list)) )\n",
    "        print( \" = \" + str(total_reward) + \" = total reward\")\n",
    "        if total_reward > optimal_reward:\n",
    "            optimal_reward = total_reward\n",
    "            if len_optimal > len(state_list):\n",
    "                len_optimal = len(state_list)\n",
    "        lastState = int( state_list[-1] )\n",
    "        r,c = state_to_coordinates(lastState)\n",
    "        if flappyworld1[r][c] == \"goal\" and total_reward > goal_optimal_reward:\n",
    "            goal_optimal_reward = total_reward\n",
    "            if len_goal_optimal > len(state_list):\n",
    "                len_goal_optimal = len(state_list)\n",
    "\n",
    "        print()\n",
    "    print(\"=\"*60)\n",
    "    print(str(len(visited_state)) + \" traversed states : \", end=\" \")\n",
    "    visited_state = [ str(state) for state in visited_state]\n",
    "    print(visited_state)\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1-(a)\n",
    " - PART1 : briefly explain what the optimal policy would be in Flappy World 1.   \n",
    " - PART2 : is the optimal policy unique ?\n",
    " - PART3 : does the optimal policy depend on the value of the discount factor $\\gamma \\in [0, 1]$?  \n",
    " - PART4 : Explain your answer.\n",
    "\n",
    "### Hints\n",
    " - What is the optimal policy? (the one that has the highest discounted sum of reward.)\n",
    " - What is the difference between positive vs negative reward values of $r_s$ ?\n",
    " - Unique or not, list conditions / why you think it is unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview : **WITHOUT** considering the discount factor $\\gamma$\n",
    "We first get an overview by traverseing all possible paths from the starting state to a terminal state (either RED or GREEN) .\n",
    " - root : starting state 2\n",
    " - leaf nodes : terminal nodes\n",
    "\n",
    "``` python\n",
    "r_s  = -4\n",
    "print(\"=\"*30+\"r_s = \" + str(r_s)+\"=\"*30)\n",
    "reward = { \"terminal\" : r_r, \"goal\" : r_g, \"unshaded\" : r_s }\n",
    "printArray(Paths(2),reward)\n",
    "```\n",
    "```\n",
    "==============================r_s = -4==============================\n",
    "path : 2 ---> 8 : length of shortest path : 2\n",
    "label : unshaded ---> terminal\n",
    "reward : -4 + -5 = -9 = total reward\n",
    "\n",
    "path : 2 ---> 10 ---> 16 : length of shortest path : 3\n",
    "label : unshaded ---> unshaded ---> terminal\n",
    "reward : -4 + -4 + -5 = -13 = total reward\n",
    "\n",
    "path : 2 ---> 10 ---> 18 ---> 24 ---> 30 ---> 31 ---> 32 : length of shortest path : 7\n",
    "label : unshaded ---> unshaded ---> unshaded ---> unshaded ---> unshaded ---> unshaded ---> goal\n",
    "reward : -4 + -4 + -4 + -4 + -4 + -4 + 5 = -19 = total reward\n",
    "\n",
    "path : 2 ---> 10 ---> 18 ---> 24 ---> 32 : length of shortest path : 5\n",
    "label : unshaded ---> unshaded ---> unshaded ---> unshaded ---> goal\n",
    "reward : -4 + -4 + -4 + -4 + 5 = -11 = total reward\n",
    "\n",
    "path : 2 ---> 10 ---> 18 ---> 26 ---> 32 : length of shortest path : 5\n",
    "label : unshaded ---> unshaded ---> unshaded ---> unshaded ---> goal\n",
    "reward : -4 + -4 + -4 + -4 + 5 = -11 = total reward\n",
    "\n",
    "path : 2 ---> 10 ---> 18 ---> 26 ---> 34 ---> 35 : length of shortest path : 6\n",
    "label : unshaded ---> unshaded ---> unshaded ---> unshaded ---> unshaded ---> terminal\n",
    "reward : -4 + -4 + -4 + -4 + -4 + -5 = -25 = total reward\n",
    "\n",
    "============================================================\n",
    "12 traversed states :  ['32', '2', '34', '35', '8', '10', '16', '18', '24', '26', '30', '31']\n",
    "============================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation from the route traversal\n",
    " - there are 12 explored states:\n",
    "     - staring state : 2\n",
    "     - RED terminal states : 8, 16, 35\n",
    "     - GREEN terminal state : 32 \n",
    "     - UNSHADED states : 10, 18, 24, 26, 30, 31, 34\n",
    " - there are 6 distinct paths from the starting state 2 (root) to a terminal state (leaf node).\n",
    "     - path A : $2 \\rightarrow 8$\n",
    "     - path B : $2 \\rightarrow 10 \\rightarrow 16$\n",
    "     - path C : $2 \\rightarrow 10 \\rightarrow 18 \\rightarrow 24 \\rightarrow 30 \\rightarrow 31 \\rightarrow 32$\n",
    "     - path D : $2 \\rightarrow 10 \\rightarrow 18 \\rightarrow 24 \\rightarrow 32$\n",
    "     - path E : $2 \\rightarrow 10 \\rightarrow 18 \\rightarrow 26 \\rightarrow 32$\n",
    "     - path F : $2 \\rightarrow 10 \\rightarrow 18 \\rightarrow 26 \\rightarrow 34 \\rightarrow 35$\n",
    "\n",
    "| path | length | $R_{acc}$ | $r_s = -4$ | $r_s = -1$ | $r_s = 0$ | $r_s = 1$ | ending state | \n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| A | 1 | $1*r_s + r_r = 1r_s - 5$ | -9 (max) | -6 | -5 | -4 | RED | \n",
    "| B | 2 | $2*r_s + r_r = 2r_s - 5$ | -13 | -7 | -5 | -3 | RED | \n",
    "| C | 6 | $6*r_s + r_g = 6r_s + 5$ | -19 | -1 | 5 (max) | 11 (max) | GREEN | \n",
    "| D | 4 | $4*r_s + r_g = 4r_s + 5$ | -11 | 1 (max) | 5 (max) | 9 | GREEN | \n",
    "| E | 4 | $4*r_s + r_g = 4r_s + 5$ | -11 | 1 (max) | 5 (max) | 9 | GREEN | \n",
    "| F | 5 | $5*r_s + r_r = 5r_s - 5$ | -25 | -10 | -5 | 0 | RED | \n",
    "\n",
    "Let $R_{acc}$ represents the reward accumulated along the path.\n",
    "\n",
    "A policy is a function that maps S to A.  \n",
    "Let $\\searrow$ represents the action \"right and down\".  \n",
    "Let $\\nearrow$ represents the action \"right and up\".  \n",
    "We represent the function as a dictionary, where the key is the state, and the value is the action.\n",
    "\n",
    "| path | policy | states | \n",
    "| --- | --- | --- |\n",
    "| A | {2:$\\nearrow$} | $2 \\rightarrow 8$ |\n",
    "| C | {2:$\\searrow$,10:$\\searrow$,18:$\\nearrow$,24:$\\nearrow$} | $2 \\rightarrow 10 \\rightarrow 18 \\rightarrow 24 \\rightarrow 30 \\rightarrow 31 \\rightarrow 32$ |\n",
    "| D | {2:$\\searrow$,10:$\\searrow$,18:$\\nearrow$,24:$\\searrow$} | $2 \\rightarrow 10 \\rightarrow 18 \\rightarrow 24 \\rightarrow 32$ |\n",
    "| E | {2:$\\searrow$,10:$\\searrow$,18:$\\searrow$,26:$\\nearrow$} | $2 \\rightarrow 10 \\rightarrow 18 \\rightarrow 26 \\rightarrow 32$ |\n",
    "\n",
    "Without considering the discount factor $\\gamma$, the optimal policy(ies) corresponds(x) to the path(s) that renders(x) the max $R_{acc}$.\n",
    "\n",
    "| $r_s$ | optimal path | unique? | \n",
    "| --- | --- | --- |\n",
    "| $-4$ | A | unique |\n",
    "| $-1$ | D & E | not unique |\n",
    "| $0$ | C & D & E | not unique |\n",
    "| $1$ | C | unique |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Paths (Policies)\n",
    "We can categorize paths into 2 types.\n",
    " - $Type_G$ : Paths end at a **GREEN** terminal state.  \n",
    " - $Type_R$ : Paths end at a **RED** terminal state.\n",
    "\n",
    "Define $L_{Gmax} = \\max_{P \\in Type_G} |P|$, the longest length among all paths that are in $Type_G$  \n",
    "Define $L_{Gmin} = \\min_{P \\in Type_G} |P|$, the shortest length among all paths that are in $Type_G$  \n",
    "Define $L_{Rmax} = \\max_{P \\in Type_R} |P|$, the longest length among all paths that are in $Type_G$  \n",
    "Define $L_{Rmin} = \\min_{P \\in Type_R} |P|$, the shortest length among all paths that are in $Type_G$  \n",
    "\n",
    "Here, the length of path equals to the number of actions in a policy excluding action taken at the terminal state.  \n",
    "\n",
    "| path | length | $R_{acc}$ | $r_s = -4$ | $r_s = -1$ | $r_s = 0$ | $r_s = 1$ | ending state | type | \n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| A | 1 | $1*r_s + r_r = 1r_s - 5$ | -9 (max) | -6 | -5 | -4 | RED | type R | \n",
    "| B | 2 | $2*r_s + r_r = 2r_s - 5$ | -13 | -7 | -5 | -3 | RED |  type R | \n",
    "| C | 6 | $6*r_s + r_g = 6r_s + 5$ | -19 | -1 | 5 (max) | 11 (max) | GREEN |   type G | \n",
    "| D | 4 | $4*r_s + r_g = 4r_s + 5$ | -11 | 1 (max) | 5 (max) | 9 | GREEN |   type G | \n",
    "| E | 4 | $4*r_s + r_g = 4r_s + 5$ | -11 | 1 (max) | 5 (max) | 9 | GREEN |  type G | \n",
    "| F | 5 | $5*r_s + r_r = 5r_s - 5$ | -25 | -10 | -5 | 0 | RED |  type R | \n",
    "\n",
    "$L_{Gmax} = \\max_{P \\in Type_G} |P| = \\max \\{|A|,|B|,|F|\\}= \\max \\{1,2,5\\} = 5$     \n",
    "$L_{Gmin} = \\min_{P \\in Type_G} |P| = \\min \\{|A|,|B|,|F|\\}= \\min \\{1,2,5\\} = 1$     \n",
    "$L_{Rmax} = \\max_{P \\in Type_R} |P| = \\max \\{|C|,|D|,|E|\\}= \\max \\{6,4,4\\} = 6$   \n",
    "$L_{Rmin} = \\min_{P \\in Type_R} |P| = \\min \\{|C|,|D|,|E|\\}= \\min \\{6,4,4\\} = 4$  \n",
    "\n",
    "\n",
    "if $r_s > 0, R_{acc} = \\max \\{ L_{Rmax}*r_s+r_r,L_{Gmax}*r_s+r_g\\}= \\max \\{ 5r_s-5,6r_s+5\\}= \\max \\{ 0,r_s+10\\}-5+5r_s$  \n",
    "if $r_s = 0, R_{acc} = \\max \\{ r_r,r_g\\}= \\max \\{ -5,5\\}$    \n",
    "if $r_s < 0, R_{acc} = \\max \\{ L_{Rmin}*r_s+r_r,L_{Gmin}*r_s+r_g\\}= \\max \\{ 1r_s-5,4r_s+5\\}= \\max \\{ 0,3r_s+10\\}-5+r_s$   \n",
    "\n",
    "if $r_s > 0, r_s+10$ is larger $ \\rightarrow $ longest Type_G path(s) is(are) optimal  \n",
    "if $r_s = 0, r_g$ is larger $ \\rightarrow $  all Type_G path(s) is(are) optimal  \n",
    "if $r_s < 0$,   \n",
    " - if $3r_s+10 > 0$ : $3r_s+10$ is larger $ \\rightarrow $ shortest Type_G path(s) is(are) optimal    \n",
    " - if $3r_s+10 = 0$ : $ \\rightarrow $ shortest path(s) is(are) optimal      \n",
    " - if $3r_s+10 < 0$ : 0 is larger $ \\rightarrow $ shortest Type_R path(s) is(are) optimal      \n",
    "\n",
    "\n",
    "\n",
    "if $r_s > 0$, longest path(s) that ends(x) at the GREEN state is(are) optimal   \n",
    "if $r_s = 0$, all path(s) that ends(x) at the GREEN state is(are) optimal  \n",
    "if $r_s < 0$,   \n",
    " - if $r_s > -10/3$ : shortest path(s) is(are) that ends(x) at the GREEN state is optimal    \n",
    " - if $r_s = -10/3$ : shortest path(s) is(are) optimal      \n",
    " - if $r_s < -10/3$ : shortest path(s) that ends(x) at a RED state is(are) optimal   \n",
    "\n",
    "\n",
    "$r_s = 1 > 0$, path C is the longest path that ends at the GREEN state $ \\rightarrow $  path C is optimal.   \n",
    "$r_s = 0$, path C,D,E end at the GREEN state $ \\rightarrow $  path C,D,E are optimal.    \n",
    "$r_s = -1 > -10/3$ : path D,E are the shortest paths that end at the GREEN state $ \\rightarrow $  path D,E, are optimal.    \n",
    "$r_s = -4 < -10/3$ : path A is the shortest path that ends at a RED state $ \\rightarrow $  path A is optimal.\n",
    "\n",
    "\n",
    "We may preview question 1-(b)\n",
    "```\n",
    "What value of r_s from 1-(a) would cause the optimal policy to return the shortest path to the green target square in all cases?\n",
    "``` \n",
    "```\n",
    "Answer : r_s = -1\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction of the search space\n",
    "There are 12 explored states:\n",
    "    - staring state : 2\n",
    "    - RED terminal states : 8, 16, 35\n",
    "    - GREEN terminal state : 32 \n",
    "    - UNSHADED states : 10, 18, 24, 26, 30, 31, 34\n",
    "\n",
    "For a terminal state $s_{terminal}$ , since taking an action on $s_{terminal}$ will end the episode, there is NO next state s’ to transition to.   \n",
    "Consequently, formula $V_{k}^{\\pi} (s) = R(s) + \\gamma \\sum_{s'} P(s'|s) V_{k-1}^{\\pi} (s')$  can be reduced to $V_{k}^{\\pi} (s_{terminal}) = R(s_{terminal})$  \n",
    "This implies that $V_{k}^{\\pi} (s_{terminal})$ will remain invariant after the 1st iteration.  \n",
    "    - RED : $V_{k}^{\\pi} (s_{red}) = R(s_{red}) = r_r = -5, \\forall k > 1$   \n",
    "    - GREEN : $V_{k}^{\\pi} (s_{green}) = R(s_{green}) = r_g = 5, \\forall k > 1$   \n",
    "     \n",
    "So, there is no need to take terminal states into the calculation of convergence as their values/utilities remain invariant $V^{\\pi}(s_{terminal}) = R(s_{terminal})$.  \n",
    "    - RED : $V^{\\pi} (s_{red}) = R(s_{red}) = r_r = -5$   \n",
    "    - GREEN : $V^{\\pi} (s_{green}) = R(s_{green}) = r_g = 5$ \n",
    "\n",
    "Also, we know that, \n",
    "1. taking any action on state 31 will result in a transition to state 32  \n",
    "$V_1(31) = R(31) + \\gamma * P(32|31) * V_{0}(32)$  \n",
    "$V_1(31) = R(31) + \\gamma * P(32|31) * r_g$  \n",
    "$V_1(31) = R(31) + \\gamma * r_g$  \n",
    "$V_1(31) = r_s + \\gamma *  r_g$   \n",
    "$V(31) = r_s + \\gamma * r_g$ is a constant  \n",
    "\n",
    "2. taking any action on state 30 will result in a transition to state 31  \n",
    "$V_1(30) = R(30) + \\gamma * P(32|31) * V_0(31)$  \n",
    "$V_1(30) = R(31) + \\gamma * P(32|31) * (r_s + \\gamma * r_g)$  \n",
    "$V_1(30) = R(31) + \\gamma * (r_s + \\gamma * r_g)$  \n",
    "$V_1(30) = r_s + \\gamma *  (r_s + \\gamma * r_g)$     \n",
    "$V_1(30) = r_s * (1 + \\gamma) + \\gamma^2 * r_g$      \n",
    "$V(30) = r_s * (1 + \\gamma) + \\gamma^2 * r_g$ is a constant    \n",
    " \n",
    "\n",
    "3. taking any action on state 34 will result in a transition to state 35   \n",
    "$V_1(34) = R(34) + \\gamma * P(35|34) * V_0(35)$  \n",
    "$V_1(34) = R(31) + \\gamma * P(32|31) * r_r$  \n",
    "$V_1(34) = R(31) + \\gamma * r_r$   \n",
    "$V_1(34) = r_s + \\gamma * r_r$     \n",
    "$V(34) = r_s + \\gamma * r_r$ is a constant    \n",
    "\n",
    "\n",
    "Therefore, the only states we need to consider in the Policy Iteration Algorithm are : [2, 10, 18, 24, 26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.zeros((N_ROW,N_COL), dtype='U1')\n",
    "value = np.zeros((N_ROW,N_COL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5., -5., -5., -5., -5.],\n",
       "       [ 0.,  0., -5.,  0.,  0.],\n",
       "       [ 0.,  0., -5.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  5.],\n",
       "       [ 0., -5.,  0.,  0.,  0.],\n",
       "       [ 0., -5.,  0.,  0.,  0.],\n",
       "       [-5., -5., -5., -5., -5.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for row in range(N_ROW):\n",
    "    for col in range(N_COL):\n",
    "        if label[row][col] == \"terminal\":\n",
    "            value[row][col] = -5\n",
    "            policy[row][col] = \"T\"\n",
    "        elif label[row][col] == \"goal\":\n",
    "            value[row][col] = 5\n",
    "            policy[row][col] = \"G\"\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp(gamma,rs):\n",
    "    # For state that will run into a wall after taking an action.\n",
    "    for row in range(N_ROW-1,-1,-1):\n",
    "        isTerminal = (label[row][N_COL-1] == \"terminal\")\n",
    "        isGoal = (label[row][N_COL-1] == \"goal\")\n",
    "        if not (isTerminal or isGoal):\n",
    "            value[row][N_COL-1] = rs + gamma * value[row+1][N_COL-1]\n",
    "            policy[row][N_COL-1] = \"W\"\n",
    "\n",
    "    # For other states.\n",
    "    for col in range(N_COL-2,-1,-1):\n",
    "        for row in range(N_ROW-1,-1,-1):\n",
    "            label_ = label[row][col]\n",
    "            isTerminal = (label_ == \"terminal\")\n",
    "            isGoal = (label_ == \"goal\")\n",
    "            if not (isTerminal or isGoal):\n",
    "                valueUp = value[row-1][col+1]\n",
    "                valueDown = value[row+1][col+1]\n",
    "                if valueUp > valueDown:\n",
    "                    policy[row][col] = \"U\" #going \"right&Uown\" is optimal\n",
    "                elif valueUp < valueDown:\n",
    "                    policy[row][col] = \"D\" #going \"right&Down\" is optimal\n",
    "                else:\n",
    "                    policy[row][col] = \"E\" #Either going \"right&up\" or going \"right&down\" is optimal\n",
    "                value[row][col] = rs + gamma * max(valueUp,valueDown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the path under the policy starting from the start state.\n",
    " - if there is a state when \"E\" occurs, meaning that either going R&U or R&D is optimal, then, the policy is NOT unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printPath(startstate,policy,tiebreaker):\n",
    "    isUnique = True\n",
    "    path = [str(startstate)]\n",
    "    row, col = state_to_coordinates(startstate)\n",
    "    while True:\n",
    "        if policy[row][col] == \"U\":\n",
    "            row-=1\n",
    "            col+=1\n",
    "        elif policy[row][col] == \"D\":\n",
    "            row+=1\n",
    "            col+=1\n",
    "        elif policy[row][col] == \"E\":\n",
    "            row+=tiebreaker\n",
    "            col+=1\n",
    "            isUnique = False\n",
    "        elif policy[row][col] == \"W\":\n",
    "            row+=1\n",
    "        path.append( str( coordinates_to_state ( row, col ) ) )\n",
    "        if label[row][col] == \"terminal\" or label[row][col] == \"goal\":\n",
    "            break\n",
    "    if isUnique:\n",
    "        print(\"the path is unique.\")\n",
    "    else:\n",
    "        print(\"the path is NOT unique.\")\n",
    "    print (\"--->\".join(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-(a)-1. Optimal Policy under $r_s = -4$\n",
    " - \"T\" means a terminal state, any action will end the episode.\n",
    " - \"G\" means the goal state, any action will end the episode.\n",
    " - \"E\" means either going \"Right and Up\" or \"Right and Down\" is optimal\n",
    " - \"U\" means either going \"Right and Up\"  is optimal\n",
    " - \"D\" means either going \"Right and Down\" is optimal\n",
    " - \"W\" means that tacking an action from this state will result in hitting a wall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['T', 'T', 'T', 'T', 'T'],\n",
       "       ['U', 'E', 'T', 'D', 'W'],\n",
       "       ['E', 'D', 'T', 'D', 'W'],\n",
       "       ['D', 'U', 'E', 'U', 'G'],\n",
       "       ['D', 'T', 'U', 'U', 'W'],\n",
       "       ['E', 'T', 'U', 'D', 'W'],\n",
       "       ['T', 'T', 'T', 'T', 'T']], dtype='<U1')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = -4\n",
    "gamma = 0.9\n",
    "dp(gamma,rs)\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-(a)-1. Path (sequence of states) from start state $s_2$ to a terminal state under the optimal policy under $r_s = -4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the path is unique.\n",
      "2--->8\n"
     ]
    }
   ],
   "source": [
    "printPath(2,policy,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-(a)-2. Optimal Policy under $r_s = -1$\n",
    " - \"T\" means a terminal state, any action will end the episode.\n",
    " - \"G\" means the goal state, any action will end the episode.\n",
    " - \"E\" means either going \"Right and Up\" or \"Right and Down\" is optimal\n",
    " - \"U\" means either going \"Right and Up\"  is optimal\n",
    " - \"D\" means either going \"Right and Down\" is optimal\n",
    " - \"W\" means that tacking an action from this state will result in hitting a wall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['T', 'T', 'T', 'T', 'T'],\n",
       "       ['D', 'E', 'T', 'D', 'W'],\n",
       "       ['D', 'D', 'T', 'D', 'W'],\n",
       "       ['U', 'D', 'E', 'U', 'G'],\n",
       "       ['U', 'T', 'U', 'U', 'W'],\n",
       "       ['E', 'T', 'U', 'D', 'W'],\n",
       "       ['T', 'T', 'T', 'T', 'T']], dtype='<U1')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = -1\n",
    "gamma = 0.9\n",
    "dp(gamma,rs)\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-(a)-2. Path (sequence of states) from start state $s_2$ to a terminal state under the optimal policy under $r_s = -1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the path is NOT unique.\n",
      "2--->10--->18--->26--->32\n"
     ]
    }
   ],
   "source": [
    "startState = 2\n",
    "printPath(startState, policy,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-(a)-3. Optimal Policy under $r_s = 0$\n",
    " - \"T\" means a terminal state, any action will end the episode.\n",
    " - \"G\" means the goal state, any action will end the episode.\n",
    " - \"E\" means either going \"Right and Up\" or \"Right and Down\" is optimal\n",
    " - \"U\" means either going \"Right and Up\"  is optimal\n",
    " - \"D\" means either going \"Right and Down\" is optimal\n",
    " - \"W\" means that tacking an action from this state will result in hitting a wall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['T', 'T', 'T', 'T', 'T'],\n",
       "       ['D', 'E', 'T', 'D', 'W'],\n",
       "       ['D', 'D', 'T', 'D', 'W'],\n",
       "       ['U', 'D', 'E', 'U', 'G'],\n",
       "       ['U', 'T', 'U', 'U', 'W'],\n",
       "       ['E', 'T', 'U', 'U', 'W'],\n",
       "       ['T', 'T', 'T', 'T', 'T']], dtype='<U1')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = 0\n",
    "gamma = 0.9\n",
    "dp(gamma,rs)\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-(a)-3. Path (sequence of states) from start state $s_2$ to a terminal state under the optimal policy under $r_s = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the path is NOT unique.\n",
      "2--->10--->18--->26--->32\n"
     ]
    }
   ],
   "source": [
    "startState = 2\n",
    "printPath(startState, policy,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-(a)-4. Optimal Policy under $r_s = 1$\n",
    " - \"T\" means a terminal state, any action will end the episode.\n",
    " - \"G\" means the goal state, any action will end the episode.\n",
    " - \"E\" means either going \"Right and Up\" or \"Right and Down\" is optimal\n",
    " - \"U\" means either going \"Right and Up\"  is optimal\n",
    " - \"D\" means either going \"Right and Down\" is optimal\n",
    " - \"W\" means that tacking an action from this state will result in hitting a wall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['T', 'T', 'T', 'T', 'T'],\n",
       "       ['D', 'E', 'T', 'D', 'W'],\n",
       "       ['D', 'D', 'T', 'U', 'W'],\n",
       "       ['U', 'D', 'U', 'U', 'G'],\n",
       "       ['U', 'T', 'U', 'U', 'W'],\n",
       "       ['E', 'T', 'U', 'U', 'W'],\n",
       "       ['T', 'T', 'T', 'T', 'T']], dtype='<U1')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = 1\n",
    "gamma = 0.9\n",
    "dp(gamma,rs)\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-(a)-4. Path (sequence of states) from start state $s_2$ to a terminal state under the optimal policy under $r_s = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the path is unique.\n",
      "2--->10--->18--->24--->30--->31--->32\n"
     ]
    }
   ],
   "source": [
    "startState = 2\n",
    "printPath(startState, policy,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-(b)\n",
    "Consider different possible grids and grid shading (with walls at the border similar to Flappy World 1) in\n",
    "which the green target square is reacheable from the starting square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-(b)-1. \n",
    "What value of rs from part (a) would cause the optimal policy to return the shortest path to the green target square in all cases?  \n",
    "**$r_s = -1$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-(b)-2. \n",
    "Find the optimal value function for each square in Flappy World 1 using this value of rs? i.e. show the value functions for each square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.     , -5.     , -5.     , -5.     , -5.     ],\n",
       "       [-0.1585 , -5.5    , -5.     ,  2.15   ,  2.15   ],\n",
       "       [-1.14265,  0.935  , -5.     ,  3.5    ,  3.5    ],\n",
       "       [-0.1585 , -0.1585 ,  2.15   ,  2.15   ,  5.     ],\n",
       "       [-1.14265, -5.     ,  0.935  ,  3.5    , -5.95   ],\n",
       "       [-5.5    , -5.     ,  2.15   , -5.5    , -5.5    ],\n",
       "       [-5.     , -5.     , -5.     , -5.     , -5.     ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = -1\n",
    "gamma = 0.9\n",
    "dp(gamma,rs)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-(b)-3.\n",
    "What is the optimal action from square 27?  \n",
    "**GO RIGHT AND DOWN** to $S_{35}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['T', 'T', 'T', 'T', 'T'],\n",
       "       ['D', 'E', 'T', 'D', 'W'],\n",
       "       ['D', 'D', 'T', 'D', 'W'],\n",
       "       ['U', 'D', 'E', 'U', 'G'],\n",
       "       ['U', 'T', 'U', 'U', 'W'],\n",
       "       ['E', 'T', 'U', 'D', 'W'],\n",
       "       ['T', 'T', 'T', 'T', 'T']], dtype='<U1')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the path is unique.\n",
      "27--->35\n"
     ]
    }
   ],
   "source": [
    "startState = 27\n",
    "printPath(startState, policy,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-(c) [5 points (Written)]\n",
    "Now consider Flappy World 2.   \n",
    "It is the same as Flappy World 1,   \n",
    "except there are no walls on the right and left sides.  \n",
    "Going past the right end of Flappy World 2 simply loops you to left hand side.   \n",
    "Take a look at Figure 2b for a successful run by Karel in Flappy World 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $r_s \\in \\{-4, -1, 0, 1\\}$.  \n",
    "\n",
    "```python\n",
    "[\"1\",\"8\",\"15\",\"22\",\"29\"],\n",
    "[\"2\",\"9\",\"16\",\"23\",\"30\"],\n",
    "[\"3\",\"10\",\"17\",\"24\",\"31\"],\n",
    "[\"4\",\"11\",\"18\",\"25\",\"32\"],\n",
    "[\"5\",\"12\",\"19\",\"26\",\"33\"],\n",
    "[\"6\",\"13\",\"20\",\"27\",\"34\"],\n",
    "[\"7\",\"14\",\"21\",\"28\",\"35\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - starting from $s_{2}$, we can see that $s_{20}$ and $s_{23}$ will never be visited; therefore, we exclude them from the discussion.  \n",
    " - $V(s_{terminal}) = R(s_{terminal}) = r_r = -5$  \n",
    " - $V(s_{goal}) = R(s_{goal}) = r_g = 5$  \n",
    " - $V(s_{9}) = R(s_{9}) + \\gamma * R(s_{15}) = r_s -5 * \\gamma$\n",
    " - $V(s_{6}) = R(s_{12}) + \\gamma * R(s_{12}) = r_s -5 * \\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5. , -5. , -5. , -5. , -5. ],\n",
       "       [ 0. , -8.5, -5. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. , -5. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  5. ],\n",
       "       [ 0. , -5. ,  0. ,  0. ,  0. ],\n",
       "       [-8.5, -5. ,  0. ,  0. ,  0. ],\n",
       "       [-5. , -5. , -5. , -5. , -5. ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_array = np.array(\n",
    "    [\n",
    "        [\"terminal\",\"terminal\",\"terminal\",\"terminal\",\"terminal\"],\n",
    "        [\"unshaded\",\"invariant\",\"terminal\",\"unvisited\",\"unshaded\"],\n",
    "        [\"unshaded\",\"unshaded\",\"terminal\",\"unshaded\",\"unshaded\"],\n",
    "        [\"unshaded\",\"unshaded\",\"unshaded\",\"unshaded\",\"goal\"],\n",
    "        [\"unshaded\",\"terminal\",\"unshaded\",\"unshaded\",\"unshaded\"],\n",
    "        [\"invariant\",\"terminal\",\"unvisited\",\"unshaded\",\"unshaded\"],\n",
    "        [\"terminal\",\"terminal\",\"terminal\",\"terminal\",\"terminal\"]\n",
    "    ]\n",
    ")\n",
    "# shape of Flappy World 1. \n",
    "N_ROW = label_array.shape[0] #7\n",
    "N_COL = label_array.shape[1] #5\n",
    "\n",
    "# state_array : state space\n",
    "S = []\n",
    "for row in range(N_ROW):\n",
    "    for col in range(N_COL):\n",
    "        if label_array[row][col] == \"unshaded\":\n",
    "            S.append((row,col))\n",
    "S = np.asarray(S)\n",
    "nS = len(S) # size = 16\n",
    "\n",
    "# action_space : action space\n",
    "A = [False,True] #False means DOWN; True means Up; \n",
    "nA = len(A) # size = 2\n",
    "\n",
    "# hyper-parameters for the policy iteration algorithm\n",
    "gamma, rs, rg, rr = 0.9, -4, 5,-5\n",
    "\n",
    "value_array = np.zeros( ( N_ROW, N_COL ) )\n",
    "for row in range(N_ROW):\n",
    "    for col in range(N_COL):\n",
    "        label = label_array[row][col]\n",
    "        if label == \"goal\":\n",
    "            value_array[row][col] = rg\n",
    "        elif  label == \"terminal\":\n",
    "            value_array[row][col] = rr\n",
    "        elif  label == \"invariant\":\n",
    "            value_array[row][col] = rs + gamma * rr\n",
    "value_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamics/Transitions for **NON-TERMINAL** states.\n",
    "\n",
    "```\n",
    "Simulate the transition for NON-TERMINAL states in Flappy World 3.\n",
    "\n",
    "Args:\n",
    "        state (int) : starting state (from-state) of the transition\n",
    "        action (bool) : action taken on the starting state : False means DOWN; True means UP\n",
    "\n",
    "Returns:\n",
    "        (int) : row index of ending state (to-state) of the transition\n",
    "        (int) : column index of ending state (to-state) of the transition\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition ( row_1, col_1, action ):\n",
    "    # A = [False,True] #False means DOWN; True means Up; \n",
    "    if action:\n",
    "        return row_1 - 1, ((col_1 + 1) % N_COL)\n",
    "    else:\n",
    "        return row_1 + 1, ((col_1 + 1) % N_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation\n",
    "```python\n",
    "Evaluate the value function from a given policy.\n",
    "\n",
    "Args:\n",
    "        policy (np.array[nS]): The policy to evaluate. Maps states to actions.\n",
    "        tol (float): Terminate policy evaluation when max |value_function(s) - prev_value_function(s)| < tol\n",
    "\n",
    "Returns:\n",
    "        value_function (np.ndarray[nS]): \n",
    "        The value function of the given policy, \n",
    "        where value_function[s] is the value of state s.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation( value_function, state_space, policy, rs, gamma=0.9, tol=1e-3 ):\n",
    "    prev_value_function = np.copy( value_function )\n",
    "    while True:\n",
    "        for fromStateIndex, action in enumerate(policy):\n",
    "            fromRow, fromCol = state_space[ fromStateIndex ]\n",
    "            value_function[ fromRow, fromCol ] = rs + gamma * prev_value_function[ transition ( fromRow, fromCol, action ) ]\n",
    "        \n",
    "        # Terminate policy evaluation when max |value_function(s) - prev_value_function(s)| < tol\n",
    "        if np.max( np.abs(value_function - prev_value_function)) < tol:\n",
    "            break\n",
    "        \n",
    "        prev_value_function = np.copy( value_function )\n",
    "    \n",
    "    return value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement\n",
    "```python\n",
    "Given the value function from policy improve the policy.\n",
    "\n",
    "    Args:\n",
    "            value_from_policy (np.ndarray): The value calculated from the policy\n",
    "            policy (np.array): The previous policy\n",
    "\n",
    "    Returns:\n",
    "            new_policy (np.ndarray[nS]): An array of integers. Each integer is the optimal\n",
    "            action to take in that state according to the environment dynamics and the\n",
    "            given value function.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement( valueFunction, stateSpace, actionSpace, policy, rs, gamma=0.9):\n",
    "    Q_ = np.zeros( ( 2, len(stateSpace) ) )\n",
    "    # Q_[0] : for DOWN Q-Value\n",
    "    # Q_[1] : for UP Q-value\n",
    "\n",
    "    for action in actionSpace:\n",
    "        for fromStateIndex, fromState in enumerate(stateSpace):\n",
    "            fromRow, fromCol = fromState\n",
    "            Q_[int(action)][fromStateIndex] = rs + gamma * valueFunction[ transition ( fromRow, fromCol, action ) ]\n",
    "    return Q_[0]<Q_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completePolicy(S,p_pi):\n",
    "    policy = np.zeros((N_ROW,N_COL), dtype='U1')\n",
    "    for i in range(N_ROW):\n",
    "        for j in range(N_COL):\n",
    "            policy[i,j] = \"_\"\n",
    "    for index, action in enumerate(p_pi):\n",
    "        fromRow, fromCol = S[index]\n",
    "        policy[fromRow, fromCol] = \"U\" if action else \"D\"\n",
    "    print(policy)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    " - (deterministic) Bellman backup for a particular policy $\\pi$, this is a part of policy evaluation because this is trying to figure out how good is a particular policy in a decision process.) \n",
    "    - Iteration of Policy Evaluation : $V^{\\pi}_{k}(s) = R^{\\pi}(s) + \\gamma  \\sum_{s' \\in S} P^{\\pi}(s'|s)V^{\\pi}_{k-1}(s')$\n",
    " - optimal policy :  $\\pi^{*} (s) = \\arg \\max_{\\pi} V^{\\pi(s)}$\n",
    "    - There exists a unique optimal value function, but the optimal policy MAY NOT be unique.\n",
    " - Policy Search :\n",
    "    - Number of deterministic policies = $|A|^{|S|} = 2^{12} = 4096$ \n",
    " - Policy Iteration : \n",
    "    - set i = 0\n",
    "    - initialize $\\pi_{0} (s)$ randomly for all states s.\n",
    "    - while i == 0 or $\\lVert \\pi_{i} - \\pi_{i-1} \\rVert_{1} > 0$\n",
    "      - $V^{\\pi_{i}} \\leftarrow$ MDP V function policy **evaluatio** of $\\pi_{i}$\n",
    "      - $\\pi_{i+1} \\leftarrow $ policy **improvement**\n",
    "      - i $\\leftarrow$ i + 1\n",
    " - State-action value of a policy $\\pi$\n",
    "    - $Q^{\\pi}(s,a) = R(s,a) + \\gamma * \\sum_{s \\in S} P(s'|s) V^{\\pi}(s')$\n",
    " - Compute State-action value of a policy $\\pi_{i}$\n",
    "    - For $s \\in S, a \\in A: $  \n",
    "      $Q^{\\pi_{i}}(s,a) = R(s,a) + \\gamma * \\sum_{s \\in S} P(s'|s) V^{\\pi_{i}}(s')$\n",
    " - Compute new policy $\\pi_{i+1},  \\forall s \\in S$:  \n",
    "    - $\\pi_{i+1}(s) = argmax_{a} Q^{\\pi_{i}}(s,a), \\forall s \\in S$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration( value, stateSpace, actionSpace, rs, gamma=0.9, tol=1e-3):\n",
    "    policy = np.zeros( len(stateSpace), dtype=bool )\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        value = policy_evaluation( value, stateSpace, policy, rs, gamma, tol )\n",
    "        policy_improved = policy_improvement( value, stateSpace, actionSpace, policy, rs, gamma )\n",
    "        \n",
    "        if np.sum(policy_improved != policy) == 0:\n",
    "            break\n",
    "        \n",
    "        policy = policy_improved\n",
    "\t\t\n",
    "    return value, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting in square 3,   \n",
    "for each of the possible values of rs:   \n",
    "#### 1-(c)-1. Briefly explain what the optimal policy would be in Flappy World 2?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_policy(rs,startRow,startCol):\n",
    "    print( \"The value function is\" )\n",
    "    print(\"=\"*30)\n",
    "    V_pi, p_pi = policy_iteration( value_array, S, A, rs, gamma=0.9, tol=1e-3)\n",
    "    print(V_pi)\n",
    "    print(\"=\"*30)\n",
    "    print( \"The policy is\" )\n",
    "    print(\"=\"*30)\n",
    "    p_pi = completePolicy(S,p_pi)\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    if (rs<0):\n",
    "        row, col = startRow, startCol\n",
    "        path = [ str(coordinates_to_state(row,col) ) ]\n",
    "        reward = V_pi[row,col]\n",
    "        \n",
    "        while True:\n",
    "            action = p_pi[row][col]\n",
    "            if action == \"U\":\n",
    "                row -= 1\n",
    "                col = (col+1)% N_COL\n",
    "                reward += V_pi[row,col]\n",
    "                path.append( str(coordinates_to_state(row,col) ) )\n",
    "            elif action == \"D\":\n",
    "                row += 1\n",
    "                col = (col+1)% N_COL\n",
    "                reward += V_pi[row,col]\n",
    "                path.append( str(coordinates_to_state(row,col) ) )\n",
    "            else:\n",
    "                break\n",
    "        print( \" ---> \".join(path) ) \n",
    "        print(\"total reward = \")\n",
    "        print(reward)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case 1 : $r_s = -4$  \n",
    "The policy is to ends through a RED terminal state.\n",
    "\n",
    "```python\n",
    "show_policy(-4,1,0)\n",
    "\n",
    " - The value function is\n",
    "===========================================\n",
    "[[ -5.     -5.     -5.     -5.     -5.   ]\n",
    " [ -8.5    -8.5    -5.      0.     -8.5  ]\n",
    " [-11.65   -7.195  -5.      0.5   -11.65 ]\n",
    " [ -8.5    -8.5    -3.55  -14.485   5.   ]\n",
    " [ -8.5    -5.    -11.65    0.5   -11.65 ]\n",
    " [ -8.5    -5.      0.     -8.5    -8.5  ]\n",
    " [ -5.     -5.     -5.     -5.     -5.   ]]\n",
    "===========================================\n",
    "\n",
    " - The policy is\n",
    "========================\n",
    "[['_' '_' '_' '_' '_']\n",
    " ['U' '_' '_' '_' 'U']\n",
    " ['D' 'D' '_' 'D' 'D']\n",
    " ['D' 'U' 'D' 'D' '_']\n",
    " ['D' '_' 'D' 'U' 'D']\n",
    " ['_' '_' '_' 'D' 'D']\n",
    " ['_' '_' '_' '_' '_']]\n",
    "========================\n",
    "one of the paths is : 3 ---> 11 ---> 17\n",
    "total reward = -25.15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case 2 : $r_s = -1$\n",
    "```python\n",
    "show_policy(-1,1,0)\n",
    "\n",
    "The policy is to ends through a GREEN terminal state.\n",
    "\n",
    "The value function is  \n",
    "================================================================\n",
    "[[-5.         -5.         -5.         -5.         -5.        ]\n",
    " [-0.1585     -8.5        -5.          0.         -4.7698234 ]\n",
    " [-4.18869267  0.935      -5.          3.5        -1.14265   ]\n",
    " [-0.1585     -3.54299185  2.15       -2.028385    5.        ]\n",
    " [-4.18869267 -5.         -2.8255465   3.5        -1.14265   ]\n",
    " [-8.5        -5.          0.         -2.028385   -4.7698234 ]\n",
    " [-5.         -5.         -5.         -5.         -5.        ]]\n",
    "================================================================\n",
    "\n",
    "The policy is  \n",
    "=======================\n",
    "[['_' '_' '_' '_' '_']\n",
    " ['D' '_' '_' '_' 'D']\n",
    " ['D' 'D' '_' 'D' 'D']\n",
    " ['U' 'D' 'D' 'D' '_']\n",
    " ['U' '_' 'D' 'U' 'U']\n",
    " ['_' '_' '_' 'U' 'U']\n",
    " ['_' '_' '_' '_' '_']]\n",
    "=======================\n",
    "one of the paths is : 3 ---> 11 ---> 19 ---> 27 ---> 33 ---> 4 ---> 10 ---> 18 ---> 26 ---> 32\n",
    "total reward = -2.301766015\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case 3 : $r_s = 0$\n",
    "The policy is to ends through a GREEN terminal state.\n",
    "\n",
    "```python\n",
    "show_policy(0,1,0)\n",
    "The value function is\n",
    "================================================================\n",
    "[[-5.         -5.         -5.         -5.         -5.        ]\n",
    " [ 3.2805     -8.5        -5.          0.          1.7433922 ]\n",
    " [ 1.93710245  3.645      -5.          4.5         2.95245   ]\n",
    " [ 3.2805      2.15233605  4.05        2.657205    5.        ]\n",
    " [ 1.93710245 -5.          2.3914845   4.5         2.95245   ]\n",
    " [-8.5        -5.          0.          2.657205    1.7433922 ]\n",
    " [-5.         -5.         -5.         -5.         -5.        ]]\n",
    "================================================================\n",
    "\n",
    "The policy is\n",
    "=======================\n",
    "[['_' '_' '_' '_' '_']\n",
    " ['D' '_' '_' '_' 'D']\n",
    " ['D' 'D' '_' 'D' 'D']\n",
    " ['U' 'D' 'D' 'D' '_']\n",
    " ['U' '_' 'D' 'U' 'U']\n",
    " ['_' '_' '_' 'U' 'U']\n",
    " ['_' '_' '_' '_' '_']]\n",
    "=======================\n",
    "one of the path is : 3 ---> 11 ---> 19 ---> 27 ---> 33 ---> 4 ---> 10 ---> 18 ---> 26 ---> 32\n",
    "total reward = 32.566077995\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case 4 : $r_s = 1$\n",
    "The policy is to not ever terminate\n",
    "```python\n",
    "show_policy(1,1,0)\n",
    "\n",
    "\n",
    "The value function is\n",
    "================================================================\n",
    "[[-5.         -5.         -5.         -5.         -5.        ]\n",
    " [ 9.99928329 -8.5        -5.          0.          9.99890763]\n",
    " [ 9.99878625  9.99920366 -5.          9.99901686  9.99935496]\n",
    " [ 9.99928329  9.99865139  9.99911518  9.99941947  5.        ]\n",
    " [ 9.99878625 -5.          9.99947752  9.99901686  9.99935496]\n",
    " [-8.5        -5.          0.          9.99941947  9.99890763]\n",
    " [-5.         -5.         -5.         -5.         -5.        ]]\n",
    "================================================================\n",
    "\n",
    "The policy is\n",
    "=======================\n",
    "[['_' '_' '_' '_' '_']\n",
    " ['D' '_' '_' '_' 'D']\n",
    " ['D' 'D' '_' 'U' 'D']\n",
    " ['U' 'D' 'D' 'D' '_']\n",
    " ['U' '_' 'D' 'D' 'U']\n",
    " ['_' '_' '_' 'U' 'U']\n",
    " ['_' '_' '_' '_' '_']]\n",
    "=======================\n",
    "\n",
    "total reward will become infinity.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response to 1-(c)-1\n",
    " - **$r_s = -4 : $**  The optimal policy is to terminate through a red square \n",
    " - **$r_s = -1 : $**  The optimal policy is to terminate through a green square \n",
    " - **$r_s = 0 : $**   The optimal policy is to terminate through a green square \n",
    " - **$r_s = 1 : $** The optimal policy is to not ever terminate  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again consider different grids and grid shading   \n",
    "(without walls at either end similar to Flappy World 2)   \n",
    "in which the green target square is reacheable from the starting square.  \n",
    "\n",
    "#### 1-(c)-2. What is the value of rs that would cause the optimal policy to return the shortest path to the green target square for all cases?  \n",
    "**$r_s = -1$**  \n",
    "Explanation :   \n",
    "Though both the policie for $r_s = -1$ and \"r_s = 0\" are to terminate through a green square.  \n",
    "In $r_s = -1$, the value functions for non-terminal states are mostly NEGATIVE, leading to potential shorter path to reach the green state.    \n",
    "In $r_s = 0$, the value functions for non-terminal states are mostly POSITIVE, leading to potential longer path to reach the green state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-(c)-3. Find the optimal value for each square in Flappy World 2 using the value of rs,that would cause the optimal policy to return the shortest path to the green target square for all cases?  \n",
    "  \n",
    "For $r_s = -1$  \n",
    "```python\n",
    "show_policy(-1,5,3)\n",
    "The value function is  \n",
    "================================================================\n",
    "[[-5.         -5.         -5.         -5.         -5.        ]\n",
    " [-0.1585     -8.5        -5.          0.         -4.7698234 ]\n",
    " [-4.18869267  0.935      -5.          3.5        -1.14265   ]\n",
    " [-0.1585     -3.54299185  2.15       -2.028385    5.        ]\n",
    " [-4.18869267 -5.         -2.8255465   3.5        -1.14265   ]\n",
    " [-8.5        -5.          0.         -2.028385   -4.7698234 ]\n",
    " [-5.         -5.         -5.         -5.         -5.        ]]\n",
    "================================================================\n",
    "\n",
    "The policy is  \n",
    "=======================\n",
    "[['_' '_' '_' '_' '_']\n",
    " ['D' '_' '_' '_' 'D']\n",
    " ['D' 'D' '_' 'D' 'D']\n",
    " ['U' 'D' 'D' 'D' '_']\n",
    " ['U' '_' 'D' 'U' 'U']\n",
    " ['_' '_' '_' 'U' 'U']\n",
    " ['_' '_' '_' '_' '_']]\n",
    "=======================\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-(c)-4. What is the optimal action from square 27?  \n",
    "\n",
    "Hint:   \n",
    "There are three possible long-term behaviours,   \n",
    "1. terminate through a red square  \n",
    "2. terminate through a green square  \n",
    "3. do not ever terminate  \n",
    "\n",
    "Consider these cases when formulating the optimal policy for each value of rs.   \n",
    "\n",
    "```python\n",
    "show_policy(-1,5,3)\n",
    "The value function is\n",
    "==============================\n",
    "[[-5.         -5.         -5.         -5.         -5.        ]\n",
    " [-0.1585     -8.5        -5.          0.         -4.7698234 ]\n",
    " [-4.18869267  0.935      -5.          3.5        -1.14265   ]\n",
    " [-0.1585     -3.54299185  2.15       -2.028385    5.        ]\n",
    " [-4.18869267 -5.         -2.8255465   3.5        -1.14265   ]\n",
    " [-8.5        -5.          0.         -2.028385   -4.7698234 ]\n",
    " [-5.         -5.         -5.         -5.         -5.        ]]\n",
    "==============================\n",
    "The policy is\n",
    "==============================\n",
    "[['_' '_' '_' '_' '_']\n",
    " ['D' '_' '_' '_' 'D']\n",
    " ['D' 'D' '_' 'D' 'D']\n",
    " ['U' 'D' 'D' 'D' '_']\n",
    " ['U' '_' 'D' 'U' 'U']\n",
    " ['_' '_' '_' 'U' 'U']\n",
    " ['_' '_' '_' '_' '_']]\n",
    "==============================\n",
    "the optimal action is to go \"right and up\" and follows the following path to reach a GREEN terminal state 32\n",
    "27 ---> 33 ---> 4 ---> 10 ---> 18 ---> 26 ---> 32\n",
    "total reward = 8.255465000000001\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

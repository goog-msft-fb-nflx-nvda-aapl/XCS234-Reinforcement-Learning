{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions Induced by a Policy\n",
    "\n",
    " - an infinite-horizon MDP M = <S, A, R, T, $\\gamma$>\n",
    "     - S : states\n",
    "     - A : actions\n",
    "     - R : rewards\n",
    "     - T : transitions\n",
    "     - $\\gamma$ : discount factor\n",
    " - stochastic policies of the form $\\pi: S \\rightarrow \\Delta(A)$  \n",
    " - For a finite set X, $\\Delta(X)$ refers to the set of categorical distributions with support on X or, equivalently, the $\\Delta^{|X|-1}$ probability simplex.}.     \n",
    " - Additionally, we'll assume that M has a single, fixed starting state $s_0 \\in S$ for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-a\n",
    "\n",
    "Suppose we have a single Markov Decision Process (MDP) and two policies for that MDP, π1 and π2.\n",
    "Naturally, we are often interested in the performance of policies obtained in the MDP, quantified by V^{π1} and V^{π2} , respectively.\n",
    "If the reward function and transition dynamics of the underlying MDP are known to us, we can use standard methods for policy evaluation.\n",
    "There are many scenarios, however, where the underlying MDP model is not known and we must try to infer something about the performance of policy π2 solely based on data obtained through executing policy π1 within the environment.\n",
    "In this problem, we will explore a classic result for quantifying the gap in performance between two policies that only requires access to data sampled from one of the policies.\n",
    "\n",
    "Consider \n",
    "1. an infinite-horizon MDP M = ⟨S, A, R, P, γ⟩.\n",
    "\t1-1. S is a set of Markov states s in S\n",
    "\t1-2. A is a set of actions a in A\n",
    "\t1-3. R is a reward function\n",
    "\t1-4. P is the dynamics/transition model for each action, that specifies P(st+1 =s′|st =s,at =a)\n",
    "\t1-5. γ is the discount factor\n",
    "2. stochastic policies of the form π : S → ∆(A)\n",
    "\tFor a finite set X, the notation ∆(X) refers to the set of categorical distributions with support on X or, equivalently, the ∆^{|X|−1} probability simplex.\n",
    "\n",
    "Specifically, π(a|s) refers to the probability of taking action a in state s, and \\sum_{a} π(a|s) = 1, ∀s.\n",
    "For simplicity, we’ll assume that this decision process has a single, fixed starting state s0 in S.\n",
    "\n",
    "Consider a fixed stochastic policy and imagine running several rollouts of this policy within the environment.\n",
    "Naturally, depending on the stochasticity of the MDP M and the policy itself, some trajectories are more likely than others.\n",
    "Question:\n",
    "Write down an expression for ρ^{π}(τ), the probability of sampling a trajectory τ = (s0, a0, s1, a1, . . .) from running policy π in the MDP M.\n",
    "To put this distribution in context, recall that $V^{π} (s0) = \\mathbb{E}_{τ~ρ^{π}} [ \\sum_{t=0}^{t=\\infty} γ^t * R(s_t,a_t) | s0 ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response 1-(a)\n",
    "\n",
    "The likelihood of sampling a trajectory $\\tau = (s_0,a_0,s_1,a_1,\\ldots)$ by running a stochastic policy $\\pi$ in the MDP $M$ is given by the product of the probabilities of each state-action pair along the trajectory:\n",
    "\n",
    "$\\rho^{\\pi}(\\tau) = P(s_0) \\prod_{t=0}^{\\infty} \\pi(a_t | s_t) P(s_{t+1} | s_t, a_t) $\n",
    "\n",
    "The product is taken over time, representing the sequential nature of the trajectory.\n",
    "\n",
    "Breaking it down:\n",
    " - $P(s_0)$ is the probability of the starting state $s_0$ under the policy $\\pi$.\n",
    " - $ \\pi(a_t | s_t)$ is the probability of taking action $a_t$ in state $s_t$ under the policy $\\pi$.\n",
    " - $P(s_{t+1} | s_t, a_t)$ is the probability of transitioning to state $s_{t+1}$ given that action $a_t$ was taken in state $s_t$ under the dynamics of the MDP.\n",
    "\n",
    "These probabilities are determined by the stochastic policy $\\pi$ and the transition dynamics $T$ of the MDP. The expression represents the likelihood of the entire trajectory $\\tau$ under the policy $\\pi$.\n",
    "\n",
    "The expectation in the value function $V^{π} (s0) = \\mathbb{E}_{τ~ρ^{π}} [ \\sum_{t=0}^{t=\\infty} γ^t * R(s_t,a_t) | s0 ]$ involves summing over all possible trajectories weighted by their likelihoods, which is why the distribution of trajectories $\\rho^\\pi$ is needed for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is $p^{\\pi}(s_t = s)$, where $p^{\\pi}(s_t = s)$ denotes the probability of being in state s at timestep t while following policy π? (Provide an equation)\n",
    "\n",
    "$p^{\\pi}(s_t = s) = \\sum_{\\tau} \\rho^{\\pi}(\\tau) \\cdot \\mathbb{I}(s_t = s)$\n",
    "\n",
    "\n",
    "$\\rho^{\\pi}(\\tau)$ is the probability of sampling the trajectory $\\tau$ under policy $\\pi$.  \n",
    "$\\mathbb{I}(s_t = s)$ is the indicator function, which is 1 if $s_t = s$ and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-(b)\n",
    "\n",
    "Just as ρ^π captures the distribution over trajectories induced by policy π, we can also examine the distribution over states induced by policy π.\n",
    "In particular, define the \"discounted, stationary state distribution\" of a policy π as, $$d^{π}(s) = (1-γ) \\sum\\limits_{t=0}^\\infty γ^t p(s_t = s),$$ where $p(s_t = s)$ denotes the probability of being in state $s$ at timestep t while following policy π.\n",
    "Your answer to the previous part should help you reason about how you might compute this value.  \n",
    "\n",
    "The value function of a policy π can be expressed using this distribution d^{π}(s,a) = d^{π}(s) * π(a|s) over states and actions, which will shortly be quite useful.\n",
    "\n",
    "Consider an arbitrary function $f: S \\times A \\rightarrow R$.   \n",
    "Prove the following identity:   \n",
    "$$\\mathbb{E}_{τ \\sim ρ^π} \\left [\\sum\\limits_{t=0}^\\infty γ^t f(s_t,a_t)\\right] = 1/(1-γ)* \\mathbb{E}_{s \\sim d^π}[ \\mathbb{E}_{ a ~ π(s) }[f(s,a)] ]$$ \n",
    "\n",
    "Hint 0: You may find it helpful to first consider how things work out for f(s, a) = 1, for all (s, a) in S × A.  \n",
    "Hint 1: Using your answer from part (a), try to understand the following identity $\\mathbb{E}_{\\tau \\sim \\rho^{\\pi}}[1[s_{t}=s]] = p(s_{t} = s)$}.  \n",
    "Hint 2: Recall the linearity property of the expectation operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response 1-(b)\n",
    "\n",
    "First, let's consider the case when $ f(s, a) = 1 $ for all $ (s, a) \\in S \\times A$.\n",
    "#### Case 1: $f(s, a) = 1$\n",
    "\n",
    "The left-hand side (LHS) of the identity is:\n",
    "$\n",
    "\\mathbb{E}_{\\tau \\sim \\rho^{\\pi}} \\left [\\sum_{t=0}^{\\infty} \\gamma^t \\cdot 1 \\right]\n",
    "$\n",
    "\n",
    "Using Hint 1, we know that $\\mathbb{E}_{\\tau \\sim \\rho^{\\pi}}[1[s_{t}=s]] = p(s_{t} = s)$.  \n",
    "Substituting this in, we get:\n",
    "$\\mathbb{E}_{\\tau \\sim \\rho^{\\pi}} \\left [\\sum_{t=0}^{\\infty} \\gamma^t \\cdot 1[s_{t}=s] \\right] = \\sum_{t=0}^{\\infty} \\gamma^t \\cdot p(s_{t} = s)$\n",
    "\n",
    "Now, let's look at the right-hand side (RHS) of the identity:\n",
    "$\\frac{1}{1-\\gamma} \\cdot \\mathbb{E}_{s \\sim d^{\\pi}} [1]$\n",
    "\n",
    "The inner expectation $\\mathbb{E}_{a \\sim \\pi(s)}[1]$ is just the sum of probabilities of all actions in state s according to policy $\\pi$, which is equal to 1.   \n",
    "Therefore, the RHS simplifies to:\n",
    "$\\frac{1}{1-\\gamma} \\cdot \\mathbb{E}_{s \\sim d^{\\pi}} [1]$\n",
    "\n",
    "Now, the term $\\mathbb{E}_{s \\sim d^{\\pi}} [1]$ represents the sum of discounted probabilities of being in state s under policy $\\pi$, which is equivalent to the expression we obtained for the LHS.   \n",
    "Therefore, in the case when $f(s, a) = 1$, the identity holds.\n",
    "\n",
    "\n",
    "\n",
    "#### Case 2: Arbitrary $f: S \\times A \\rightarrow \\mathbb{R}$\n",
    "\n",
    "Now, let's extend the proof to an arbitrary function \\(f: S \\times A \\rightarrow \\mathbb{R}\\).\n",
    "\n",
    "The LHS of the identity is:\n",
    "$\n",
    "\\mathbb{E}_{\\tau \\sim \\rho^{\\pi}} \\left [\\sum_{t=0}^{\\infty} \\gamma^t \\cdot f(s_t, a_t) \\right]\n",
    "$\n",
    "\n",
    "Using linearity of the expectation operator, we can express this as:\n",
    "$\n",
    "\\sum_{t=0}^{\\infty} \\gamma^t \\cdot \\mathbb{E}_{\\tau \\sim \\rho^{\\pi}}[f(s_t, a_t)]\n",
    "$\n",
    "\n",
    "Now, applying the result from Case 1 for the expectation $\\mathbb{E}_{\\tau \\sim \\rho^{\\pi}}[1[s_{t}=s]] = p(s_{t} = s)$, we have:\n",
    "$\n",
    "\\sum_{t=0}^{\\infty} \\gamma^t \\cdot \\sum_{s \\in S} p(s_t = s) \\cdot \\mathbb{E}_{a \\sim \\pi(s)}[f(s, a)]\n",
    "$\n",
    "\n",
    "We konw the definition of the discounted, stationary state distribution is $$d^{π}(s) = (1-γ) \\sum\\limits_{t=0}^\\infty γ^t p(s_t = s),$$  \n",
    "By this definision, we rearrange the expression of LFS to\n",
    "$\n",
    "\\frac{1}{1-\\gamma} \\sum_{s \\in S} d^{\\pi}(s) \\cdot \\mathbb{E}_{a \\sim \\pi(s)}[f(s, a)]\n",
    "$\n",
    "\n",
    "Finally, the RHS of the identity is:\n",
    "$\n",
    "\\frac{1}{1-\\gamma} \\cdot \\mathbb{E}_{s \\sim d^{\\pi}} \\left[ \\mathbb{E}_{a \\sim \\pi(s)}[f(s, a)] \\right]\n",
    "$\n",
    "\n",
    "Comparing the LHS and RHS expressions, we see that the identity holds for arbitrary $f: S \\times A \\rightarrow \\mathbb{R}$. QED."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-(c)\n",
    "\n",
    "For any policy $\\pi$, we define the following function $$A^\\pi(s,a) = Q^\\pi(s,a) - V^\\pi(s)$$.  \n",
    "$A^\\pi(s,a)$ is known as the advantage function and shows up in a lot of policy gradient based RL algorithms.  \n",
    "Intuitively, it is the additional benefit one gets from first following action a and then following π, instead of always following π.\n",
    "\n",
    "\n",
    "Prove the following statement holds for all policies $\\pi,\\pi'$: \n",
    "\n",
    "$$V^\\pi(s_0) - V^{\\pi'}(s_0) = \\frac{1}{(1-\\gamma)} \\mathbb{E}_{s \\sim d^\\pi}\\left[\\mathbb{E}_{a \\sim \\pi(s)}\\left[A^{\\pi'}(s,a)\\right]\\right]$$\n",
    "\n",
    "\n",
    "Hint 1: Recall the \"tower property of expectation\" which says that E[X] = E[E[X|Y]]\n",
    "\n",
    "Hint 2: $$\\mathbb{E}_{τ \\sim ρ^π} \\left [\\sum\\limits_{t=0}^\\infty γ^t f(s_t,a_t)\\right] = 1/(1-γ)* \\mathbb{E}_{s \\sim d^π}[ \\mathbb{E}_{ a \\sim π(s) }[f(s,a)] ]$$ \n",
    "\n",
    "Note: We have provided you with scaffolding for your derivation. In the provided scaffold we have substituted in our expression for $V^{\\pi}(s_{0})$ from part (a). In addition, we have reexpressed $V^{\\pi'}(s_{0})$ as a telescoping sum.  \n",
    "\n",
    "$V^\\pi(s_0) - V^{\\pi'}(s_0) = \\mathbb{E}_{\\tau \\sim \\rho^\\pi}\\left[\\sum\\limits_{t=0}^\\infty \\gamma^t \\mathcal{R}(s_t,a_t)\\right] - V^{\\pi'}(s_0) $  \n",
    "$= \\mathbb{E}_{\\tau \\sim \\rho^\\pi}\\left[\\sum\\limits_{t=0}^\\infty \\gamma^t\\left( \\mathcal{R}(s_t,a_t) + V^{\\pi'}(s_t) - V^{\\pi'}(s_t)\\right)\\right] - V^{\\pi'}(s_0) $  \n",
    "$= \\mathbb{E}_{\\tau \\sim \\rho^\\pi}\\left[\\sum\\limits_{t=0}^\\infty \\gamma^t\\left( \\mathcal{R}(s_t,a_t) + \\gamma V^{\\pi'}(s_{t+1}) - V^{\\pi'}(s_t)\\right)\\right] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response 1-(c)\n",
    "\n",
    "\\begin{align*}\n",
    "V^\\pi(s_0) - V^{\\pi'}(s_0) &= \\mathbb{E}_{\\tau \\sim \\rho^\\pi}\\left[\\sum\\limits_{t=0}^\\infty \\gamma^t \\mathcal{R}(s_t,a_t)\\right] - V^{\\pi'}(s_0) \\\\\n",
    "&= \\mathbb{E}_{\\tau \\sim \\rho^\\pi}\\left[\\sum\\limits_{t=0}^\\infty \\gamma^t\\left( \\mathcal{R}(s_t,a_t) + V^{\\pi'}(s_t) - V^{\\pi'}(s_t)\\right)\\right] - V^{\\pi'}(s_0) \\\\\n",
    "&= \\mathbb{E}_{\\tau \\sim \\rho^\\pi}\\left[\\sum\\limits_{t=0}^\\infty \\gamma^t\\left( \\mathcal{R}(s_t,a_t) + \\gamma V^{\\pi'}(s_{t+1}) - V^{\\pi'}(s_t)\\right)\\right] \\\\\n",
    "&= \\mathbb{E}_{\\tau \\sim \\rho^\\pi}\\left[\\sum\\limits_{t=0}^\\infty \\gamma^t\\left( Q^{\\pi'}(s_t,a_t) - V^{\\pi'}(s_t)\\right)\\right] \\\\\n",
    "&= \\mathbb{E}_{\\tau \\sim \\rho^\\pi}\\left[\\sum\\limits_{t=0}^\\infty \\gamma^t\\left( A^\\pi(s_t,a_t)\\right)\\right] \\\\\n",
    "&= 1/(1-γ)* \\mathbb{E}_{s \\sim d^π}[ \\mathbb{E}_{ a \\sim π(s) }[A^\\pi(s,a)] ]\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-(a) [4points]\n",
    "\n",
    "1. objective function : define $V(s,n)$ as the maximum sum of rewards starting from state s in a single trajectory/episode of length n.  \n",
    "2. recurrence relation : $\\max_{a} ( r(s,a) + V( s', n-1 ) )$\n",
    "    - taking an action **a** at state **s** will result in transitioning to state **s'** with an immediate reward **r(s,a)**  \n",
    "3. boundary conditions : $V(s,0) = 0, \\forall s$  \n",
    "4. represent output as the objective function : $V(0,5)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Problem2:\n",
    "    def __init__(self):\n",
    "        self.transition = np.array([\n",
    "            [[0, 0.2], [1, -0.1], [2, 0.0], [3, -0.3], [0, 0.2]],\n",
    "            [[0, 0.2], [1, -0.1], [2, 0.0], [3, -0.3], [1, -0.1]],\n",
    "            [[0, -2.0], [1, 1.0], [2, 0.0], [3, 3.0], [2, 0.0]],\n",
    "            [[0, 0.2], [1, -0.1], [2, 0.0], [3, -0.3], [3, -0.3]]\n",
    "        ])\n",
    "        self.nextState = np.asarray( self.transition[:,:,0],dtype=int)\n",
    "        self.immediateReward = np.asarray( self.transition[:,:,1],dtype=float)\n",
    "        self.nA = 5 # number of actions\n",
    "        self.nS = 4 # number of states\n",
    "    \n",
    "    \"\"\"\n",
    "    Input Arguments\n",
    "        s0 : (int) : the starting state\n",
    "        L : (int) : the length of the episode/trajectory.\n",
    "    Returned Output:\n",
    "        max_reward : float : the maximum sum of rewards starting from state s in a single trajectory/episode of length L.\n",
    "        opt_actions : (list) of (int) : a sequence of states, representing the trajectory that renders the max_reward.\n",
    "    \"\"\"\n",
    "    def DP ( self, s0, L ):\n",
    "        valueTable = np.full( ( self.nS, L+1 ), float('-inf'))\n",
    "        actionTable = np.zeros( ( self.nS, L+1 ), dtype=int )\n",
    "\n",
    "        # boundary conditions\n",
    "        if L == 0:\n",
    "            return 0, []\n",
    "        for state in range(self.nS):\n",
    "            valueTable[state][0] = 0\n",
    "        \n",
    "        ### PART 1 TABLE FILLING ###\n",
    "        # len_ : 1, 2, 3, ... L\n",
    "        for len_ in range( 1, L+1 ):\n",
    "            # state : 0, 1, 2, 3\n",
    "            for state in range(self.nS):\n",
    "                # action : 0, 1, 2, 3, 4\n",
    "                for action in range(self.nA):\n",
    "                    next_state = self.nextState[state][action]\n",
    "                    immediate_reward = self.immediateReward[state][action]\n",
    "                    if immediate_reward + valueTable[next_state][len_-1] > valueTable[state][len_]:\n",
    "                        valueTable[state][len_] = immediate_reward + valueTable[next_state][len_-1]\n",
    "                        actionTable[state][len_] = action\n",
    "        \n",
    "        ### PART 2 BACK-TRACKING ###\n",
    "        optimal_actions = [0] * L\n",
    "        state = s0\n",
    "\n",
    "        for timestamp in range(L):\n",
    "            opt_action = actionTable[state][L-timestamp]\n",
    "            next_state = self.nextState[state][opt_action]\n",
    "            optimal_actions[ timestamp ] = opt_action\n",
    "            state = next_state\n",
    "\n",
    "        return valueTable[s0][L], optimal_actions\n",
    "\n",
    "    def get_traj( self, start_state, optimal_actions ):\n",
    "        current_state = start_state\n",
    "        trajectory = []\n",
    "\n",
    "        for action_index in optimal_actions:\n",
    "            next_state, reward = self.transition[current_state][action_index]\n",
    "            trajectory.append({\n",
    "                'state_t': current_state,\n",
    "                'action_t': action_index,\n",
    "                'reward_t': reward\n",
    "            })\n",
    "            current_state = int(next_state)\n",
    "\n",
    "        return trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - maximum sum of rewards starting from state 0 in a single trajectory/episode of length 5 is 6.2\n",
      " - this value is attainable in a single trajectorythe by following the optimal sequence of actions: [0, 2, 3, 2, 3]\n",
      " - the trajectory of following the optimal actions can be represented as st, at, Rt.\n",
      "\tTimestamp 0 - State: 0, Action: 0, Reward: 0.2\n",
      "\tTimestamp 1 - State: 0, Action: 2, Reward: 0.0\n",
      "\tTimestamp 2 - State: 2, Action: 3, Reward: 3.0\n",
      "\tTimestamp 3 - State: 3, Action: 2, Reward: 0.0\n",
      "\tTimestamp 4 - State: 2, Action: 3, Reward: 3.0\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5\n",
    "starting_state = 0\n",
    "\n",
    "p = Problem2()\n",
    "max_sum_of_rewards, optimal_actions = p.DP(0,num_steps)\n",
    "trajectory = p.get_traj(starting_state, optimal_actions)\n",
    "\n",
    "print(\"\\n - maximum sum of rewards starting from state \" +str(starting_state)+ \" in a single trajectory/episode of length \" + str(num_steps) + \" is \" + str(max_sum_of_rewards) )\n",
    "print(\" - this value is attainable in a single trajectorythe by following the optimal sequence of actions:\", optimal_actions)\n",
    "print(\" - the trajectory of following the optimal actions can be represented as st, at, Rt.\")\n",
    "for t, step in enumerate(trajectory):\n",
    "    print(f\"\\tTimestamp {t} - State: {step['state_t']}, Action: {step['action_t']}, Reward: {step['reward_t']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument for why no other trajectory can achieve a greater cumulative reward is based on the design of the dynamic programming algorithm.\n",
    "The algorithm explores all possible trajectories from the initial state with a fixed number of steps, ensuring that all potential paths are considered.\n",
    " - **the upper bound of the cumulated reward in a single trajectory**  \n",
    "    - the maximum reward in a single step we can obtain is 3, by taking action \"3\" at state \"2\" then transitioning to state \"3\".\n",
    "    - having transitioned to state \"3\", we need to transition back to state \"2\" from state \"3\" in order to obtain the maximum single-step reward 3 again.\n",
    "    - under our scenario, we can get  at most $\\lfloor {n_{step}/2} \\rfloor$ = $\\lfloor {5/2} \\rfloor$ = 2 times of this max single-step reward.\n",
    "    - by the above induction, the maximum reward we can get in 4 steps is 2 * $r_{max}$ = +6. ( --> state 2 --(+3)--> state 3 --> state 2 --(+3)--> state 3)\n",
    "    - the max reward starting from state 0 is 0.2 \n",
    "     ```\n",
    "     np.max( self.immediateReard[0,:,:] )\n",
    "     ```\n",
    "    - Consequently, the overall upper bound of the cumulated reward in a single episode is 6 + 0.2 = 6.2.  \n",
    "    - fact 1 : no trajectory can have a cumulated reward larger than 6.2.\n",
    "    - fact 2: our optimal trajectory has a cumulated reward of 6.2.\n",
    "    - conclusion : no other trajectory can achieve greater cumulative reward than the one we get, i.e. 6.2.\n",
    " - **Optimal Substructure**:  \n",
    "    The dynamic programming approach considers the optimal substructure property, meaning that the optimal solution to the problem can be constructed from the optimal solutions of its subproblems. In this case, the optimal trajectory for a given state and remaining steps is built upon the optimal trajectories for its successor states with fewer steps.  \n",
    "     - suppose the start state is $s_0$, and length of the episode is $L$.  \n",
    "     - since a list of actions maps to a trajectory, we will discuss list of actions in the following discussion.  \n",
    "     - suppose the optimal list of actions is P, and ${s_0,s_1,s_2,...s_{L}}$ is the sequence of states if we follow P.  \n",
    "     - Let's consider an action $a_t$ on P for some t between 0 and $L-1$.  \n",
    "     - What can we say for certain about cumulated reward from $s_0$ to $s_t$ and the cumulated reward from $s_{t+1}$ to $s_L$?  \n",
    "     - For certain, we can say that the path from $s_0$ to $s_1$ must be the maximum cumulated reward of a trajectory of length $t-1$ from $s_0$ to $s_{t}$, and similarly the cumulated reward from $s_{t+1}$ to $s_{L}$ must be \"a\" possible path from $s_{t+1}$ to $s_{L}$ with max cumulated reward.  \n",
    "     - Why?  We could argue by contradiction.  \n",
    "     - Suppose that there was a path from $s_0$ to $s_t$ with larger cumulated reward.  \n",
    "     - Then we could take that path, then follow the path from k to v and we would obtain a shorter path than P from u to v. This would contradict that u to v is a shortest path.\n",
    "     is one and then consider any vertex k on the P somewhere between u and v.\n",
    "     For certain, we can say that the accumulated from u to k must be a shortest possible path from u to k, and similarly the path from k to v must be a shortest possible path from k to v.\n",
    " - **Maximizing Cumulative Reward**:  \n",
    "    At each step, the algorithm chooses the action that maximizes the cumulative reward. By doing this for every step in the trajectory, the algorithm is guaranteed to find the trajectory with the maximum cumulative reward among all possibilities.\n",
    " - **Backtracking and Memoization**:  \n",
    "    The algorithm uses backtracking to explore different actions at each step and memoization to avoid redundant calculations for already explored subproblems. This ensures that each subproblem is only solved once, preventing unnecessary recomputation.\n",
    "Given these factors, the algorithm is exhaustive in its exploration and selects the trajectory with the maximum cumulative reward at each step. Therefore, by the end of the exploration, the trajectories identified with the maximum sum of rewards are guaranteed to be the optimal trajectories, and no other trajectory can achieve a greater cumulative reward within the given constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-b. Tabular Q-Learning [5 points]\n",
    "\n",
    "If the state and action spaces are sufficiently small, we can simply maintain a table containing the value of $Q(s,a)$, an estimate of $Q^*(s,a)$, for every $(s,a)$ pair.\n",
    "In this tabular setting, given an experience sample $(s, a, r, s')$, the update rule is\n",
    "\\begin{align*}\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha\\left(r + \\gamma \\max_{a' \\in \\mathcal{A}}Q(s',a') - Q(s,a)\\right)\n",
    "\\end{align*}\n",
    "where $\\alpha > 0$ is the learning rate, $\\gamma \\in [0,1)$ the discount factor.\n",
    "\n",
    "In addition, to formalizing our update rule for Q-learning in the tabular setting we must also consider strategies for exploration.  \n",
    "In this question, we will be considering an $\\epsilon$-greedy exploration strategy. This strategy means that each time we look to choose an action $A$, we will do so as follows,\n",
    "\n",
    "\\begin{align*}\n",
    "A \\leftarrow \\begin{cases}\n",
    "\t\t\t\targmax_{a \\in \\mathcal{A}} Q(s,a) \\;\\; \\text{with probability $1-\\epsilon$} \\\\\n",
    "\t\t\t\ta \\in \\mathcal{A}  \\;\\; \\text{chosen uniformly at random with probability $\\epsilon$}\n",
    "\t\t\t \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "In the following questions, you will need to implement both the update rule and $\\epsilon$-greedy exploration strategy for Q-learning in the tabular setting.\n",
    "\n",
    "We will now examine the issue of overestimation bias in Q-learning.   \n",
    "The crux of the problem is that, since we take a max over actions, errors which cause Q to overestimate will tend to be amplified when computing the target value, while errors which cause Q to underestimate will tend to be suppressed.\n",
    "\n",
    "Assume for simplicity that our Q function is an unbiased estimator of $Q^*$, meaning that $\\mathbb{E}[Q(s,a)] = Q^*(s,a)$ for all states $s$ and actions $a$.  \n",
    "Show that, even in this seemingly benign case, the estimator overestimates the real target in the following sense:\n",
    "$$\n",
    "\\forall s, \\quad \\mathbb{E}\\left[ \\max_a Q(s,a) \\right] \\ge \\max_a Q^*(s,a)\n",
    "$$\n",
    "\n",
    "\n",
    "Note: The expectation $\\mathbb{E}[Q(s,a)]$ is over the randomness in $Q$ resulting from the stochasticity of the exploration process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response 3-(b)\n",
    "Since the **max** function is [convex](https://en.wikipedia.org/wiki/Convex_function), as per the [Jensen's Ineqality](https://en.wikipedia.org/wiki/Jensen%27s_inequality#:~:text=In%20mathematics%2C%20Jensen%27s%20inequality%2C%20named,integral%20of%20the%20convex%20function.) we have : \n",
    "$\\mathbb{E}\\left[\\max_a Q(s,a)\\right] >= \\max_a \\mathbb{E}\\left[ Q(s,a)\\right]$  \n",
    "\n",
    "Since Q is an unbiased estimator of $Q^*$, we have : \n",
    "$\\max_a \\mathbb{E}\\left[ Q(s,a)\\right] = \\max_a Q^*(s,a)$.  \n",
    "\n",
    "Therefore, $\\mathbb{E}\\left[\\max_a Q(s,a)\\right] >= \\max_a \\mathbb{E}\\left[ Q(s,a)\\right] = \\max_a Q^*(s,a)$.  \n",
    "\n",
    "The expectation of the maximum over actions is greater than or equal to the maximum of the true $Q^*$ values for that state $s$.   \n",
    "This shows that, even in the case where $Q$ is an unbiased estimator of $Q^*$, the estimator overestimates the real target.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-(a) Linear Approximation\n",
    "\n",
    "Suppose we represent the Q function as\n",
    "$ Q_{\\theta}(s, a) = \\theta^\\top \\delta(s,a) $\n",
    " - $\\theta \\in \\mathbb{R}^{\\vert S \\vert \\vert A \\vert \\times 1 }$  \n",
    " - $\\delta : S \\times \\mathcal{A} \\rightarrow \\mathbb{R}^{\\vert S \\vert \\vert A \\vert \\times 1}$  \n",
    "\n",
    "$\n",
    "[\\delta(s,a)]_{s'',a''} = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } s'' = s \\text{ and } a'' = a \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response 5-(a)-1\n",
    "  1. Compute $\\nabla_{\\theta} Q_{\\theta}(s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To apply the chain rule, we consider the composition of two functions:\n",
    "\n",
    "1. The outer function $f(\\theta) = \\theta^\\top \\delta(s, a)$\n",
    "2. The inner function $g(\\theta) = \\theta$ (which is a linear mapping)\n",
    "\n",
    "The chain rule states that the derivative of the composition of two functions is the product of the derivative of the outer function and the derivative of the inner function. Mathematically, it is expressed as:\n",
    "\n",
    "$ \\nabla_{\\theta} (f \\circ g)(\\theta) = (\\nabla_{g} f)(g(\\theta)) \\cdot (\\nabla_{\\theta} g)(\\theta) $\n",
    "\n",
    "In our case:\n",
    "\n",
    "$ \\nabla_{\\theta} Q_{\\theta}(s, a) = (\\nabla_{\\theta} (\\theta^\\top \\delta(s, a)) = (\\nabla_{\\theta} (\\theta^\\top))(g(\\theta)) \\cdot (\\nabla_{\\theta} g)(\\theta) $\n",
    "\n",
    "Now, $\\nabla_{\\theta} (\\theta^\\top)$ is the derivative of the transpose of $\\theta$, which is simply the identity matrix I.\n",
    "\n",
    "So, we have:\n",
    "\n",
    "$ \\nabla_{\\theta} Q_{\\theta}(s, a) = I \\cdot \\delta(s, a) = \\delta(s, a) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, given the linear approximation $Q_{\\theta}(s, a) = \\theta^\\top \\delta(s, a)$ where $\\delta(s, a)$ is a one-hot encoded vector, we have:\n",
    "\n",
    "$\\nabla_{\\theta} Q_{\\theta}(s, a) = \\delta(s, a) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response 5-(a)-2 Write the update rule for $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Now, let's compare the tabular Q-learning update rule with the linear approximation update rule:\n",
    "\n",
    "1. **Tabular Q-learning update rule:**\n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha\\left(r + \\gamma \\max_{a' \\in \\mathcal{A}}Q(s',a') - Q(s,a)\\right)$\n",
    "\n",
    "2. **Linear approximation update rule:**\n",
    "$\\theta \\leftarrow \\theta + \\alpha\\left(r + \\gamma \\max_{a' \\in \\mathcal{A}} Q_{\\theta}(s', a') - Q_{\\theta}(s, a)\\right) \\nabla_{\\theta}Q_{\\theta}(s, a)$\n",
    "\n",
    "Now, substitute the expressions for $Q_{\\theta}(s, a)$ and $\\nabla_{\\theta} Q_{\\theta}(s, a)$ into the linear approximation update rule:\n",
    "\n",
    "$\\theta \\leftarrow \\theta + \\alpha\\left(r + \\gamma \\max_{a' \\in \\mathcal{A}} (\\theta^\\top \\delta(s', a')) - \\theta^\\top \\delta(s, a)\\right) \\delta(s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response 5-(a)-3\n",
    "  3. Argue that equation for the tabular q-learning update rule we saw before:  \n",
    "  $Q(s,a) \\leftarrow Q(s,a) + \\alpha\\left(r + \\gamma \\max_{a' \\in \\mathcal{A}}Q(s',a') - Q(s,a)\\right) $  \n",
    "  and the following equation:  \n",
    "  $ \\theta \\leftarrow \\theta + \\alpha\\left(r+\\gamma \\max_{a' \\in \\mathcal{A}} Q_{\\theta}(s', a') - Q_{\\theta}(s, a)\\right) \\nabla_{\\theta}Q_{\\theta}(s, a) $   \n",
    "  are exactly the same when this form of linear approximation is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1.   \n",
    "The update rule for $\\theta$ we got in 5-(a)-2 is the following.  \n",
    "$\\theta \\leftarrow \\theta + \\alpha\\left(r + \\gamma \\max_{a' \\in \\mathcal{A}} (\\theta^\\top \\delta(s', a')) - \\theta^\\top \\delta(s, a)\\right) \\delta(s, a)$\n",
    "\n",
    "#### case 1. $(s'',a'') \\neq (s,a)$\n",
    "In this update rule, for entry $(s'',a'') \\neq (s,a)$, we have $[\\delta(s, a)]_{(s'',a'')} = 0$   \n",
    "and thus the update rule becomes $[\\theta]_{(s'',a'')} \\leftarrow [\\theta]_{(s'',a'')}$\n",
    "\n",
    "#### case 2. $(s'',a'') = (s,a)$\n",
    "In this update rule, for entry $(s'',a'') = (s,a)$, we have $[\\delta(s, a)]_{(s'',a'')} = 1$   \n",
    "and thus the update rule becomes $[\\theta]_{(s'',a'')} \\leftarrow [\\theta]_{(s'',a'')} + \\alpha\\left(r + \\gamma \\max_{a' \\in \\mathcal{A}} (\\theta^\\top \\delta(s', a')) - \\theta^\\top \\delta(s, a)\\right)$  \n",
    "\n",
    "or equivalently,\n",
    "$[\\theta]_{(s,a)} \\leftarrow [\\theta]_{(s,a)} + \\alpha\\left(r + \\gamma \\max_{a' \\in \\mathcal{A}} (\\theta^\\top \\delta(s', a')) - \\theta^\\top \\delta(s, a)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2. Review the LINEAR APPROXIMATION of Q.  \n",
    "###Disclaimer : Here we use (s,a) to represent the ($s*|S|+a$)-entry for simplicity.\n",
    "\n",
    "Let's review the linear approximation $Q_{\\theta} (s,a) = \\theta^T\\delta(s,a) = [0,0,...,0,0,[\\theta]_{(s,a)},0,0,...0]$, which is a |S||A|-dimension row vector having all entries zero except for the (s,a)-entry.\n",
    "i.e.\n",
    "\n",
    "$[Q_{\\theta} (s,a)]_{(s'',a'')} = 0$  if $(s'',a'') \\neq  (s,a)$  \n",
    "$[Q_{\\theta} (s,a)]_{(s'',a'')} = [\\theta]_{(s,a)}$ if $(s'',a'') =  (s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3. SUMMARY\n",
    "Therefore, the last formula in STEP 1 can be further reduced to\n",
    "\n",
    "$[Q_{\\theta} (s,a)]_{(s,a)} \\leftarrow [Q_{\\theta} (s,a)]_{(s,a)}+ \\alpha\\left(r + \\gamma \\max_{a' \\in \\mathcal{A}} (\\theta^\\top \\delta(s', a')) - \\theta^\\top \\delta(s, a)\\right)$   \n",
    "\n",
    "which is exactly the same as the update rule under the tabular setting when this form of linear approximation is used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
